<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-131526811-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-131526811-1');
  </script>

  <title>Zhiheng Li's Homepage</title>

  <meta name="author" content="Zhiheng Li">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!-- <link rel="icon" type="image/png" href="images/URCS_icon.png"> -->
</head>

<body>
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Zhiheng Li</name>
                  </p>
                  <p style="text-align:center">
                    Pronunciation (<a href="https://translate.google.com/?sl=auto&tl=en&text=%E6%99%BA%E6%81%92%20%E6%9D%8E">üîä</a>): Zhiheng: / §ih…õn≈ã/, Li: /li/
                  </p>
                  <p>I am a member of technical staff on the superintelligence team at Microsoft AI, working on multimodal data. Previously, I was a Senior Applied Scientist at Amazon AGI where I led the data efforts for video generation model. Before that, I interned at Meta AI and NEC Labs America. I received my PhD degree in computer science at University of Rochester in 2023 (PhD advisor: Prof. <a href="https://www.cs.rochester.edu/~cxu22/index.html">Chenliang Xu</a>) and my bachelor's degree at Wuhan University in 2018.
                  </p>
                  <p>
                    My research interests are data-centric machine learning, computer vision, generative AI, and AI safety.
                  </p>
                  <p>
                    Email: zhiheng.li -at- ieee.org
                  </p>

                  <p style="text-align:center">
                    <a href="data/Zhiheng_Li_CV.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=CyL4cGIAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://openreview.net/profile?id=~Zhiheng_Li1">OpenReview</a> &nbsp/&nbsp
                    <a href="https://www.linkedin.com/in/zhiheng-li-ur/">LinkedIn</a> &nbsp/&nbsp
                    <a href="https://github.com/zhihengli-UR">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/UR_avatar.jpeg"><img style="width:75%;max-width:75%" alt="profile photo"
                      src="images/UR_avatar_circle.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Projects</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr>
                <td style="padding:10px;width:15%;vertical-align:middle" align="center">
                  <img src="images/nova_reel_camera_control.gif" alt="clean-usnob" width="425" height="239">
                </td>
                <td width="100%" valign="middle">
                  <a href="https://aws.amazon.com/ai/generative-ai/nova/">
                    <papertitle>
                      Amazon Nova Reel
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://www.amazon.science/publications/the-amazon-nova-family-of-models-technical-report-and-model-card">technical report</a> / <a href="https://aws.amazon.com/ai/generative-ai/nova/creative/#:~:text=study%2C%20multi%2Dangle%E2%80%9D-,Amazon%20Nova%20Reel,-%E2%80%9CA%20snowman%20in">video examples</a>
                  <br>
                  <p>I worked on the training data for Amazon Nova Reel‚ÄîAmazon's video generation foundation model, including a data-centric approach for camera motion control.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:10px;width:15%;vertical-align:middle" align="center">
                  <img src="images/themis-with-balance-weights-vectorportal.png" alt="clean-usnob" width="256" height="256">
                </td>
                <td width="100%" valign="middle">
                  <a href="https://www.amazon.science/blog/amazon-web-services-releases-two-new-titan-vision-language-models">
                    <papertitle>
                      Amazon Titan Multimodal Models
                    </papertitle>
                  </a>

                  <p>I worked on fairness for Amazon Titan Multimodal models.</p>
                </td>
              </tr>

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Publications</heading>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <!-- paper -->
              <tr>
                <td style="padding:10px;width:15%;vertical-align:middle" align="center">
                  <img src="images/FairRAG.jpg" alt="clean-usnob" width="397" height="262">
                </td>
                <td width="100%" valign="middle">
                  <a href="https://arxiv.org/abs/2403.19964">
                    <papertitle>
                      FairRAG: Fair Human Generation via Fair Retrieval Augmentation
                    </papertitle>
                  </a>
                  <br>

                  <a href="https://robikshrestha.com/">Robik Singh Shrestha</a>,
                  <a href="https://yzou2.github.io/">Yang Zou</a>,
                  Qiuyu Chen,
                  <strong>Zhiheng Li</strong>,
                  Yusheng Xie,
                  Siqi Deng
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2403.19964">pdf</a> /
                  <a href="https://arxiv.org/abs/2403.19964">arxiv</a>
                  <br>
                  <p>We propose FairRAG to mitigate demographic biases of diffusion models (e.g., Stable Diffusion) based on Retrieval Augmented Generation (RAG).</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/cvpr2024_dim.jpg" alt="clean-usnob" width="340" height="220">
                </td>
                <td width="100%" valign="middle">
                  <a href="https://arxiv.org/abs/2403.12777">
                    <papertitle>
                    Discover and Mitigate Multiple Biased Subgroups in Image Classifiers
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://zhangaipi.github.io/">Zeliang Zhang</a>,
                  <a href="https://openreview.net/profile?id=~Mingqian_Feng1">Mingqian Feng</a>,
                  <strong>Zhiheng Li (Project Lead)</strong>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://arxiv.org/pdf/2403.12777">pdf</a> /
                  <a href="https://arxiv.org/abs/2403.12777">arxiv</a> /
                  <a href="https://github.com/ZhangAIPI/DIM">code</a>
                  <br>
                  <p>We proposed a new method to discover multiple biased subgroups based on Partial Least Squares (PLS), which enables dimension reduction guided by useful supervisions from the image classifier.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/imagenet_w_teaser.jpg" alt="clean-usnob" width="374" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2212.04825">
                    <papertitle>
                      A Whac-A-Mole Dilemma: Shortcuts Come in Multiples Where Mitigating One Amplifies Others
                    </papertitle>
                  </a>
                  <br>
                  <strong>Zhiheng Li</strong>,
                  <a href="https://ivanevtimov.eu/">Ivan Evtimov</a>,
                  <a href="https://agordo.github.io/">Albert Gordo</a>,
                  <a href="https://hazirbas.com/">Caner Hazirbas</a>,
                  <a href="https://talhassner.github.io/home/">Tal Hassner</a>,
                  <a href="https://cristiancanton.github.io/">Cristian Canton Ferrer</a>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>,
                  <a href="https://markibrahim.me/">Mark Ibrahim</a>
                  <br>
                  <em>CVPR</em>, 2023
                  <br>
                  <a href="https://arxiv.org/pdf/2212.04825">pdf</a> /
                  <a href="data/bibtex/whac_a_mole.bib">bibtex</a> /
                  <a href="https://arxiv.org/abs/2212.04825">arxiv</a> /
                  <a href="https://github.com/facebookresearch/Whac-A-Mole.git">code</a> /
                  <a href="https://youtu.be/htYGHm53bJs">video</a>
                  <br>
                  <p>We introduce two new datasets (UrbanCars and ImageNet-W) to study multi-shortcut learning. Especially, ImageNet-W is created based on the newly found watermark shortcut in ImageNet affecting a broad range of vision models, including ResNet, RegNet, ViT, MoCov3, MAE, SEER, SWAG, model soups, and CLIP. Our work surfaces an overlooked challenge in shortcut learning: multi-shortcut mitigation resembles a <em>Whac-A-Mole</em> game, i.e., mitigating one shortcut amplifies others.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/eccv_2022_DebiAN.jpg" alt="clean-usnob" width="418" height="180">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1018_ECCV_2022_paper.php">
                    <papertitle>Discover and Mitigate Unknown Biases with Debiasing Alternate Networks
                    </papertitle>
                  </a>
                  <br>
                  <strong>Zhiheng Li</strong>,
                  <a href="https://www.kitware.com/anthony-hoogs/">Anthony Hoogs</a>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
                  <br>
                  <em>ECCV</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2207.10077">pdf</a> /
                  <a href="data/bibtex/li_2022_eccv.bib">bibtex</a> /
                  <a href="https://arxiv.org/abs/2207.10077">arxiv</a> /
                  <a href="https://github.com/zhihengli-UR/DebiAN">code</a> /
                  <a href="https://youtu.be/dZLFQ8VXzBw">video</a>
                  <br>
                  <p>We introduce Debiasing Alternate Networks (DebiAN) to discover and mitigate unknown biases of an image classifier. DebiAN trains two networks in an alternate fashion. The <em>discoverer</em> network identifies unknown biases in the <em>classifier</em>. The <em>classifier</em> mitigates biases found by the <em>discoverer</em>.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/eccvw_2022_good_embed.jpg" alt="clean-usnob" width="418" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://arxiv.org/abs/2212.06254">
                    <papertitle>You Only Need a Good Embeddings Extractor to Fix Spurious Correlations
                    </papertitle>
                  </a>
                  <br>
                  <a href="http://cim.mcgill.ca/~raghav/">Raghav Mehta</a>,
                  <a href="https://vitoralbiero.netlify.app/">V√≠tor Albiero</a>,
                  Li Chen,
                  <a href="https://ivanevtimov.eu/">Ivan Evtimov</a>,
                  Tamar Glaser,
                  <strong>Zhiheng Li</strong>,
                  <a href="https://talhassner.github.io/home/">Tal Hassner</a>
                  <br>
                  <em>ECCV Responsible Computer Vision Workshop</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2212.06254">pdf</a> /
                  <a href="data/bibtex/mehta_2022_eccvw.bib">bibtex</a> /
                  <a href="https://arxiv.org/abs/2212.06254">arxiv</a>
                  <br>
                  <p>Unlike existing approaches (e.g., GroupDRO) focusing on reweighting or rebalancing training data, we show that simply using embeddings from a large pretrained vision model extractor (e.g., SWAG) and training a linear classifier on top of it without training group information achieve state-of-the-art results in combating group shift on Waterbirds.</p>
                </td>
              </tr>

              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/cvpr2022_stylet2i.jpg" alt="clean-usnob" width="278" height="106">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content/CVPR2022/html/Li_StyleT2I_Toward_Compositional_and_High-Fidelity_Text-to-Image_Synthesis_CVPR_2022_paper.html">
                    <papertitle>StyleT2I: Toward Compositional and High-Fidelity Text-to-Image Synthesis
                    </papertitle>
                  </a>
                  <br>
                  <strong>Zhiheng Li</strong>,
                  <a href="https://www.cs.toronto.edu/~cuty/">Martin Renqiang Min</a>,
                  <a href="http://kailigo.github.io/">Kai Li</a>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
                  <br>
                  <em>CVPR</em>, 2022
                  <br>
                  <a href="https://arxiv.org/pdf/2203.15799.pdf">pdf</a> /
                  <a href="data/bibtex/li_2022_cvpr.bib">bibtex</a> /
                  <a href="https://arxiv.org/abs/2203.15799">arxiv</a> /
                  <a href="https://github.com/zhihengli-UR/StyleT2I">code</a> /
                  <a href="https://youtu.be/-rTDfnKoVps">video</a>
                  <br>
                  <p>We introduce a new framework, StyleT2I, to improve the compositionality of text-to-image synthesis, e.g., faithfully synthesizing a face image in ‚Äúa man wearing lipstick‚Äù that is underrepresented in the training data (e.g., due to societal stereotypes).</p>
                </td>
              </tr>
              <!-- paper -->

              <!-- paper -->
              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/iccv2021_discover_image.png" alt="clean-usnob" width="220" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Li_Discover_the_Unknown_Biased_Attribute_of_an_Image_Classifier_ICCV_2021_paper.html">
                    <papertitle>Discover the Unknown Biased Attribute of an Image Classifier
                    </papertitle>
                  </a>
                  <br>
                  <strong>Zhiheng Li</strong>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
                  <br>
                  <em>ICCV</em>, 2021
                  <br>
                  <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Li_Discover_the_Unknown_Biased_Attribute_of_an_Image_Classifier_ICCV_2021_paper.pdf">pdf</a> /
                  <a href="data/bibtex/li_2021_iccv.bib">bibtex</a> /
                  <a href="https://arxiv.org/abs/2104.14556">arxiv</a> /
                  <a href="https://github.com/zhihengli-UR/discover_unknown_biases">code</a> /
                  <a href="https://youtu.be/LIaJi1EfD0o">video</a>
                  <br>
                  <p>We study a new problem: discover the unknown bias (i.e., the bias that is out of human's conjecture) of an image classifier. We tackle it by optimizing a hyperplane in generative model's latent space. The semantic meaning of the bias can be interpreted from the variation in the synthesized traversal images based on the optimized latent hyperplane.</p>
                </td>
              </tr>
              <!-- paper -->

              <!-- paper -->
              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/cvpr2020_dgm_image.jpg" alt="clean-usnob" width="250" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Deep_Grouping_Model_for_Unified_Perceptual_Parsing_CVPR_2020_paper.html">
                    <papertitle>Deep Grouping Model for Unified Perceptual Parsing
                    </papertitle>
                  </a>
                  <br>
                  <strong>Zhiheng Li</strong>,
                  <a href="https://cn.linkedin.com/in/wenxuan-bao-343254154/en">Wenxuan Bao</a>,
                  <a href="https://www.linkedin.com/in/jiayang-zheng-262076136">Jiayang Zheng</a>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
                  <br>
                  <em>CVPR</em>, 2020
                  <br>
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_Deep_Grouping_Model_for_Unified_Perceptual_Parsing_CVPR_2020_paper.pdf">pdf</a> /
                  <a href="data/bibtex/li_2020_cvpr.bib">bibtex</a> /
                  <a href="http://arxiv.org/abs/2003.11647">arxiv</a>
                  <br>
                  <p>We propose DGM, which incorporates the traditional perceptual grouping process into modern CNN architecture for better contextual modeling, interpretability, and a lower computational overhead.</p>
                </td>
              </tr>
              <!-- paper -->

              <!-- paper -->
              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/cvpr2020_wiseselect_image.jpg" alt="clean-usnob" width="250" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Learning_a_Weakly-Supervised_Video_Actor-Action_Segmentation_Model_With_a_Wise_CVPR_2020_paper.html">
                    <papertitle>Learning a Weakly-Supervised Video Actor-Action Segmentation Model with a Wise Selection
                    </papertitle>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/jie-chen-b55544a5/">Jie Chen</a>,
                  <strong>Zhiheng Li</strong>,
                  <a href="https://www.urmc.rochester.edu/labs/maddox.aspx">Ross K Maddox</a>,
                  <a href="https://www.cs.rochester.edu/u/jluo/"> Jiebo Luo</a>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
                  <br>
                  <em>CVPR</em>, 2020 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_Learning_a_Weakly-Supervised_Video_Actor-Action_Segmentation_Model_With_a_Wise_CVPR_2020_paper.pdf">pdf</a> /
                  <a href="data/bibtex/chen_2020_cvpr.bib">bibtex</a> /
                  <a href="https://arxiv.org/abs/2003.13141">arxiv</a>
                  <br>
                  <p>A novel method to generate and evolve the pseudo-annotations for weakly-supervised video actor-action segmentation task.</p>
                </td>
              </tr>
              <!-- paper -->

              <!-- paper -->
              <tr>
                <td style="padding:20px;width:35%;vertical-align:middle" align="center">
                  <img src="images/cg_chemical_science_2020.png" alt="clean-usnob" width="250" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://pubs.rsc.org/en/content/articlelanding/2020/SC/D0SC02458A">
                    <papertitle>Graph Neural Network Based Coarse-Grained Mapping Prediction</papertitle>
                  </a>
                  <br>
                  <strong>Zhiheng Li</strong>,
                  <a href="https://geemi725.github.io/">Geemi P. Wellawatte</a>,
                  <a href="https://www.linkedin.com/in/maghesree">Maghesree Chakraborty</a>,
                  <a href="https://www.linkedin.com/in/hetagandhi">Heta A. Gandhi</a>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>,
                  <a href="https://thewhitelab.org/">Andrew D. White</a>
                  <br>
                  <em>Chemical Science</em>, 2020
                  <br>
                  <a href="https://pubs.rsc.org/en/content/articlepdf/2020/sc/d0sc02458a">pdf</a> /
                  <a href="data/bibtex/li_2020_chemical_science.bib">bibtex</a> /
                  <a href="https://github.com/rochesterxugroup/DSGPM">code</a> /
                  <a href="https://github.com/rochesterxugroup/HAM_dataset">dataset</a> /
                  <a href="https://arxiv.org/abs/2007.04921">arxiv</a>
                  <br>
                  <p>We train a GNN to predict coarse-grained (CG) molecule, i.e., graph partitioning on the given molecule. We also collect HAM dataset, which provides CG mapping annotations of molecules.</p>
                </td>
              </tr>
              <!-- paper -->

              <!-- paper -->
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle" align="center">
                  <img src="images/eccv2018.gif" alt="clean-usnob" width="160" height="160">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://openaccess.thecvf.com/content_ECCV_2018/html/Lele_Chen_Lip_Movements_Generation_ECCV_2018_paper.html">
                    <papertitle>Lip Movements Generation at a Glance</papertitle>
                  </a>
                  <br>
                  <a href="https://www.cs.rochester.edu/u/lchen63/">Lele Chen</a>*,
                  <strong>Zhiheng Li</strong>* (*Equal Contribution),
                  <a href="https://www.urmc.rochester.edu/labs/maddox.aspx">Ross K Maddox</a>,
                  <a href="http://www2.ece.rochester.edu/~zduan/"> Zhiyao Duan</a>,
                  <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
                  <br>
                  <em>ECCV</em>, 2018
                  <br>
                  <a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Lele_Chen_Lip_Movements_Generation_ECCV_2018_paper.pdf">pdf</a> /
                  <a href="https://github.com/lelechen63/3d_gan">code</a> /
                  <a href="https://www.youtube.com/watch?v=7IX_sIL5v0c&feature=emb_logo">video</a> /
                  <a href="data/bibtex/chen_2018_eccv.bib">bibtex</a> /
                  <a href="https://arxiv.org/abs/1803.10404">arxiv</a>
                  <br>
                  <p>Given an audio speech and a lip image of an arbitrary target identity, synthesize lip movements of the target identity saying the speech.</p>
                </td>
              </tr>
              <!-- paper -->

            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Experience</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/mai.webp" style="max-width: 200px; max-height: 200px"></td>
                <td width="75%" valign="center">
                  <b>Microsoft AI</b>
                  <br><br>
                  superintelligence, multimodal team
                  <br><br>
                  02/2026 - Present
                  <br><br>
                  Redmond, WA
                  <br><br>
                  Member of Technical Staff
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/amazon-logo-squid-ink-smile-orange.png" style="max-width: 200px; max-height: 200px"></td>
                <td width="75%" valign="center">
                  <b>Amazon AGI</b>
                  <br><br>
                  Vision Skill team
                  <br><br>
                  11/2023 - 01/2026
                  <br><br>
                  Bellevue, WA
                  <br><br>
                  Senior Applied Scientist
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/aws_logo.png" style="max-width: 200px; max-height: 200px"></td>
                <td width="75%" valign="center">
                  <b>AWS AI Labs</b>
                  <br><br>
                  Bedrock Multimodal Responsible AI team
                  <br><br>
                  07/2023 - 11/2023
                  <br><br>
                  Bellevue, WA
                  <br><br>
                  Applied Scientist II
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/meta_ai_logo.jpeg" style="max-width: 200px; max-height: 200px"></td>
                <td width="75%" valign="center">
                  <b>Meta AI</b>
                  <br><br>
                  Responsible AI (Robustness and Safety team)
                  <br><br>
                  05/2022 - 12/2022
                  <br><br>
                  Seattle, WA
                  <br><br>
                  Research Intern
                  <br><br>
                  Mentors: <a href="https://ivanevtimov.eu/">Ivan Evtimov</a>, <a href="https://markibrahim.me/">Mark Ibrahim</a>, <a href="https://agordo.github.io/" >Albert Gordo</a>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/nec.png" style="max-width: 200px; max-height: 200px"></td>
                <td width="75%" valign="center">
                  <b>NEC Labs America</b>
                  <br><br>
                  Machine Learning Department
                  <br><br>
                  06/2021 - 09/2021
                  <br><br>
                  Princeton, NJ
                  <br><br>
                  Research Intern
                  <br><br>
                  Mentors: <a href="http://www.cs.toronto.edu/~cuty/">Martin Renqiang Min</a>, <a
                    href="http://kailigo.github.io/">Kai Li</a>
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Education</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/UR_logo.png" width="160" height="160"></td>
                <td width="75%" valign="center">
                  <b>University of Rochester</b>
                  <br><br>
                  08/2018 - 07/2023
                  <br><br>
                  Rochester, NY
                  <br><br>
                  Ph.D. in Computer Science
                  <br><br>
                  Advisor: <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
                </td>
              </tr>
              <tr>
                <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/WHU_logo.png" width="160" height="160"></td>
                <td width="75%" valign="center">
                  <b>Wuhan University</b>
                  <br><br>
                  09/2014 - 06/2018
                  <br><br>
                  Wuhan, Hubei, China
                  <br><br>
                  B.Eng. in Software Engineering
                </td>
              </tr>
            </tbody>
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody>
              <tr>
                <td>
                  <heading>Service</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <!-- <table width="100%" align="center" border="0" cellpadding="20">
            <tbody>
              <tr>
                <td width="75%" valign="center">
                  Conference Reviewer: NeurIPS'20, CVPR'21, ICML'21, ICCV'21, NeurIPS'21, ICLR'22, AAAI'22, CVPR'22, ICML'22, ECCV'22
                  <br><br>
                  Journal Reviewer: TMLR
                  <br><br>
                  Volunteer: FAccT'21, ICLR'21
                </td>
              </tr>
            </tbody>
          </table> -->
          <table>
            <!-- <thead>

            </thead> -->
            <tbody>
              <tr>
                <th align="right">Conference Reviewer:</th>
                <td>NeurIPS'20, CVPR'21, ICML'21, ICCV'21, NeurIPS'21, ICLR'22, AAAI'22, CVPR'22, ICML'22, ECCV'22, NeurIPS'22 (top reviewer), CVPR'23, ICML'23, FAccT'23, ICCV'23, NeurIPS'23, ICLR'24, CVPR'24, ICML'24, ECCV'24, ICCV'25</td>
              </tr>
              <tr>
                <th align="right">Journal Reviewer:</th>
                <td>TMLR, TPAMI, TMM</td>
              </tr>
              <tr>
                <th align="right">Volunteer:</th>
                <td>FAccT'21, ICLR'21</td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    The template is based on <a href="http://jonbarron.info">Jon Barron</a>'s website.
                    <br>
                  </p>
                  <p style="text-align:right;font-size:small;">
                    The <a href="https://www.flaticon.com/free-icons/whack-a-mole" title="Whack-A-Mole icons">Whack-A-Mole icon is created by Flat Icons - Flaticon</a>
                    <br>
                  </p>
                  <p style="text-align:right;font-size:small;">
                    The Themis image is created by <a href=" https://www.vectorportal.com" >Vectorportal.com</a>, <a class="external text" href="https://creativecommons.org/licenses/by/4.0/" >CC BY</a>
                    <br>
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>